{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive RAG Question Answering System\n",
    "\n",
    "This notebook provides an interactive interface for asking questions about your documents using:\n",
    "- **Multilingual Bi-encoder** for initial retrieval\n",
    "- **HyDE** (Hypothetical Document Embeddings) for improved query understanding\n",
    "- **Cross-encoder reranking** for better relevance\n",
    "- **LiteLLM** for flexible LLM provider support\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Install required packages\n",
    "2. Set your API key and file path\n",
    "3. Run the setup cells\n",
    "4. Start asking questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation (run this first if packages not installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you need to install packages\n",
    "# !pip install haystack-ai sentence-transformers litellm torch ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import litellm\n",
    "from haystack import Document, Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.rankers import SentenceTransformersSimilarityRanker\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.utils import ComponentDevice\n",
    "\n",
    "# For interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG System Classes (copy from the main codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration class for RAG pipeline\"\"\"\n",
    "    # Model configurations\n",
    "    biencoder_model: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    crossencoder_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    llm_model: str = \"gemini/gemini-1.5-flash\"  # LiteLLM format\n",
    "    \n",
    "    # Processing configurations\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 50\n",
    "    top_k_retrieval: int = 20\n",
    "    top_k_reranking: int = 5\n",
    "    \n",
    "    # Device configuration\n",
    "    device: ComponentDevice = ComponentDevice.from_str(\"cuda:0\")\n",
    "    \n",
    "    # Storage paths\n",
    "    embeddings_file: str = \"document_embeddings.pkl\"\n",
    "    documents_file: str = \"documents.pkl\"\n",
    "    \n",
    "    # LLM parameters\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 1000\n",
    "\n",
    "class LiteLLMGenerator:\n",
    "    \"\"\"LiteLLM generator component for unified LLM access\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gemini/gemini-1.5-flash\", api_key: Optional[str] = None, **kwargs):\n",
    "        self.model_name = model_name\n",
    "        self.generation_kwargs = kwargs\n",
    "        \n",
    "        # Set API key if provided\n",
    "        if api_key:\n",
    "            if \"gemini\" in model_name.lower():\n",
    "                os.environ[\"GEMINI_API_KEY\"] = api_key\n",
    "            elif \"openai\" in model_name.lower() or \"gpt\" in model_name.lower():\n",
    "                os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "            elif \"anthropic\" in model_name.lower() or \"claude\" in model_name.lower():\n",
    "                os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
    "        \n",
    "        # Test the connection\n",
    "        self._test_connection()\n",
    "    \n",
    "    def _test_connection(self):\n",
    "        \"\"\"Test if the model is accessible\"\"\"\n",
    "        try:\n",
    "            response = litellm.completion(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "                max_tokens=10\n",
    "            )\n",
    "            logger.info(f\"Successfully connected to {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not test connection to {self.model_name}: {e}\")\n",
    "    \n",
    "    def run(self, prompt: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response using LiteLLM\"\"\"\n",
    "        try:\n",
    "            generation_params = {**self.generation_kwargs, **kwargs}\n",
    "            \n",
    "            response = litellm.completion(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                **generation_params\n",
    "            )\n",
    "            \n",
    "            return {\"replies\": [response.choices[0].message.content]}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response with {self.model_name}: {e}\")\n",
    "            return {\"replies\": [\"Sorry, I couldn't generate a response.\"]}\n",
    "\n",
    "class HyDEGenerator:\n",
    "    \"\"\"Hypothetical Document Embeddings (HyDE) generator\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_generator: LiteLLMGenerator):\n",
    "        self.generator = llm_generator\n",
    "        self.hyde_prompt = \"\"\"Given the following question, write a hypothetical document that would perfectly answer this question.\n",
    "The document should be detailed, informative, and directly address the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Hypothetical Document:\"\"\"\n",
    "    \n",
    "    def run(self, query: str) -> Dict[str, str]:\n",
    "        \"\"\"Generate hypothetical document for the given query\"\"\"\n",
    "        prompt = self.hyde_prompt.format(question=query)\n",
    "        result = self.generator.run(prompt)\n",
    "        hypothetical_doc = result[\"replies\"][0] if result[\"replies\"] else query\n",
    "        return {\"hypothetical_document\": hypothetical_doc}\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document processing and embedding generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.document_store = InMemoryDocumentStore()\n",
    "        \n",
    "        # Initialize embedder\n",
    "        self.document_embedder = SentenceTransformersDocumentEmbedder(\n",
    "            model=config.biencoder_model,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        self.document_embedder.warm_up()\n",
    "        \n",
    "        # Initialize splitter\n",
    "        self.splitter = DocumentSplitter(\n",
    "            split_by=\"word\",\n",
    "            split_length=config.chunk_size,\n",
    "            split_overlap=config.chunk_overlap\n",
    "        )\n",
    "    \n",
    "    def process_text_file(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Process text file into documents\"\"\"\n",
    "        logger.info(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Convert file to document\n",
    "        converter = TextFileToDocument()\n",
    "        documents = converter.run(sources=[file_path])[\"documents\"]\n",
    "        \n",
    "        # Split documents\n",
    "        split_docs = self.splitter.run(documents=documents)[\"documents\"]\n",
    "        \n",
    "        logger.info(f\"Created {len(split_docs)} document chunks\")\n",
    "        return split_docs\n",
    "    \n",
    "    def generate_embeddings(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Generate embeddings for documents\"\"\"\n",
    "        logger.info(\"Generating embeddings...\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embedded_docs = self.document_embedder.run(documents=documents)[\"documents\"]\n",
    "        \n",
    "        logger.info(f\"Generated embeddings for {len(embedded_docs)} documents\")\n",
    "        return embedded_docs\n",
    "    \n",
    "    def save_documents_and_embeddings(self, documents: List[Document]):\n",
    "        \"\"\"Save documents and embeddings to files\"\"\"\n",
    "        logger.info(\"Saving documents and embeddings...\")\n",
    "        \n",
    "        # Save documents\n",
    "        with open(self.config.documents_file, 'wb') as f:\n",
    "            pickle.dump(documents, f)\n",
    "        \n",
    "        # Save embeddings separately\n",
    "        embeddings_data = {\n",
    "            'embeddings': [doc.embedding for doc in documents],\n",
    "            'metadata': [{'id': doc.id, 'content': doc.content[:100] + '...'} for doc in documents]\n",
    "        }\n",
    "        \n",
    "        with open(self.config.embeddings_file, 'wb') as f:\n",
    "            pickle.dump(embeddings_data, f)\n",
    "        \n",
    "        logger.info(\"Documents and embeddings saved successfully\")\n",
    "    \n",
    "    def load_documents_and_embeddings(self) -> List[Document]:\n",
    "        \"\"\"Load documents and embeddings from files\"\"\"\n",
    "        logger.info(\"Loading documents and embeddings...\")\n",
    "        \n",
    "        if not os.path.exists(self.config.documents_file):\n",
    "            raise FileNotFoundError(\"Documents file not found. Please process documents first.\")\n",
    "        \n",
    "        with open(self.config.documents_file, 'rb') as f:\n",
    "            documents = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} documents\")\n",
    "        return documents\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"Main RAG pipeline with HyDE and cross-encoder reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig, api_key: str):\n",
    "        self.config = config\n",
    "        self.document_store = InMemoryDocumentStore()\n",
    "        \n",
    "        # Initialize LLM generator\n",
    "        self.llm_generator = LiteLLMGenerator(\n",
    "            model_name=config.llm_model,\n",
    "            api_key=api_key,\n",
    "            temperature=config.temperature,\n",
    "            max_tokens=config.max_tokens\n",
    "        )\n",
    "        \n",
    "        # Initialize HyDE generator\n",
    "        self.hyde_generator = HyDEGenerator(self.llm_generator)\n",
    "        \n",
    "        # Initialize retriever\n",
    "        self.retriever = InMemoryEmbeddingRetriever(\n",
    "            document_store=self.document_store,\n",
    "            top_k=config.top_k_retrieval\n",
    "        )\n",
    "        \n",
    "        # Initialize text embedder for queries\n",
    "        self.text_embedder = SentenceTransformersTextEmbedder(\n",
    "            model=config.biencoder_model,\n",
    "            device=config.device\n",
    "        )\n",
    "        \n",
    "        # Initialize cross-encoder ranker\n",
    "        self.ranker = SentenceTransformersSimilarityRanker(\n",
    "            model=config.crossencoder_model,\n",
    "            top_k=config.top_k_reranking,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        self.text_embedder.warm_up()\n",
    "        self.ranker.warm_up()\n",
    "        \n",
    "        # QA prompt template\n",
    "        self.qa_prompt_template = \"\"\"Context information is below:\n",
    "---------------------\n",
    "{% for doc in documents %}\n",
    "{{ doc.content }}\n",
    "---------------------\n",
    "{% endfor %}\n",
    "\n",
    "Given the context information above, please answer the following question.\n",
    "If the answer cannot be found in the context, please say \"I cannot find the answer in the provided context.\"\n",
    "\n",
    "Question: {{ query }}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        self.prompt_builder = PromptBuilder(template=self.qa_prompt_template)\n",
    "    \n",
    "    def load_documents(self, documents: List[Document]):\n",
    "        \"\"\"Load documents into the document store\"\"\"\n",
    "        logger.info(\"Loading documents into document store...\")\n",
    "        self.document_store.write_documents(documents)\n",
    "        logger.info(f\"Loaded {len(documents)} documents into document store\")\n",
    "    \n",
    "    def answer_question(self, query: str, use_hyde: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Answer question using the full RAG pipeline\"\"\"\n",
    "        logger.info(f\"Processing query: {query}\")\n",
    "        \n",
    "        # Step 1: Generate hypothetical document (HyDE)\n",
    "        if use_hyde:\n",
    "            logger.info(\"Generating hypothetical document (HyDE)...\")\n",
    "            hyde_result = self.hyde_generator.run(query)\n",
    "            search_query = hyde_result[\"hypothetical_document\"]\n",
    "            logger.info(f\"HyDE generated document: {search_query[:100]}...\")\n",
    "        else:\n",
    "            search_query = query\n",
    "        \n",
    "        # Step 2: Embed the search query\n",
    "        logger.info(\"Embedding search query...\")\n",
    "        query_embedding = self.text_embedder.run(text=search_query)[\"embedding\"]\n",
    "        \n",
    "        # Step 3: Retrieve documents using bi-encoder\n",
    "        logger.info(\"Retrieving documents...\")\n",
    "        retrieved_docs = self.retriever.run(\n",
    "            query_embedding=query_embedding,\n",
    "            top_k=self.config.top_k_retrieval\n",
    "        )[\"documents\"]\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(retrieved_docs)} documents\")\n",
    "        \n",
    "        # Step 4: Rerank using cross-encoder\n",
    "        logger.info(\"Reranking documents...\")\n",
    "        reranked_docs = self.ranker.run(\n",
    "            query=query,  # Use original query for reranking\n",
    "            documents=retrieved_docs\n",
    "        )[\"documents\"]\n",
    "        \n",
    "        logger.info(f\"Reranked to top {len(reranked_docs)} documents\")\n",
    "        \n",
    "        # Step 5: Generate final answer\n",
    "        logger.info(\"Generating final answer...\")\n",
    "        prompt = self.prompt_builder.run(\n",
    "            query=query,\n",
    "            documents=reranked_docs\n",
    "        )[\"prompt\"]\n",
    "        \n",
    "        answer = self.llm_generator.run(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer[\"replies\"][0],\n",
    "            \"retrieved_documents\": retrieved_docs,\n",
    "            \"reranked_documents\": reranked_docs,\n",
    "            \"hyde_document\": search_query if use_hyde else None\n",
    "        }\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Main system orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, config: Optional[RAGConfig] = None):\n",
    "        self.config = config or RAGConfig()\n",
    "        self.api_key = api_key\n",
    "        self.document_processor = DocumentProcessor(self.config)\n",
    "        self.rag_pipeline = RAGPipeline(self.config, api_key)\n",
    "    \n",
    "    def setup_from_text_file(self, file_path: str, force_reprocess: bool = False):\n",
    "        \"\"\"Setup the system from a text file\"\"\"\n",
    "        # Check if embeddings already exist\n",
    "        if (os.path.exists(self.config.embeddings_file) and \n",
    "            os.path.exists(self.config.documents_file) and \n",
    "            not force_reprocess):\n",
    "            \n",
    "            logger.info(\"Found existing embeddings, loading...\")\n",
    "            documents = self.document_processor.load_documents_and_embeddings()\n",
    "        else:\n",
    "            logger.info(\"Processing documents and generating embeddings...\")\n",
    "            # Process documents\n",
    "            documents = self.document_processor.process_text_file(file_path)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            documents = self.document_processor.generate_embeddings(documents)\n",
    "            \n",
    "            # Save for future use\n",
    "            self.document_processor.save_documents_and_embeddings(documents)\n",
    "        \n",
    "        # Load into RAG pipeline\n",
    "        self.rag_pipeline.load_documents(documents)\n",
    "        logger.info(\"System setup complete!\")\n",
    "    \n",
    "    def ask(self, question: str, use_hyde: bool = True) -> str:\n",
    "        \"\"\"Ask a question and get an answer\"\"\"\n",
    "        result = self.rag_pipeline.answer_question(question, use_hyde=use_hyde)\n",
    "        return result[\"answer\"]\n",
    "    \n",
    "    def ask_detailed(self, question: str, use_hyde: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Ask a question and get detailed results\"\"\"\n",
    "        return self.rag_pipeline.answer_question(question, use_hyde=use_hyde)\n",
    "\n",
    "print(\"✅ RAG System classes loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 CONFIGURATION - MODIFY THESE VALUES\n",
    "from my_secrets import API_KEY\n",
    "TEXT_FILE_PATH = \"/home/fiok/work/rag-test/data/tarczynski.txt\"  # Replace with your text file path\n",
    "\n",
    "# Model configuration\n",
    "config = RAGConfig(\n",
    "    llm_model=\"gemini/gemini-2.0-flash-exp\",  # Change to your preferred model\n",
    "    # Alternative models:\n",
    "    # llm_model=\"openai/gpt-4o\",\n",
    "    # llm_model=\"anthropic/claude-3-sonnet-20240229\",\n",
    "    \n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    top_k_retrieval=20,\n",
    "    top_k_reranking=5,\n",
    "    device=ComponentDevice.from_str(\"cuda:0\"),  # Change to \"cpu\" if no GPU\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"✅ Configuration loaded!\")\n",
    "print(f\"📋 Model: {config.llm_model}\")\n",
    "print(f\"🔧 Device: {config.device}\")\n",
    "print(f\"📄 Document file: {TEXT_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize and Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG system\n",
    "print(\"🚀 Initializing RAG System...\")\n",
    "rag_system = RAGSystem(API_KEY, config)\n",
    "\n",
    "# Setup from text file (this will process and embed documents if not done before)\n",
    "print(\"📚 Setting up documents...\")\n",
    "try:\n",
    "    rag_system.setup_from_text_file(TEXT_FILE_PATH, force_reprocess=False)\n",
    "    print(\"✅ RAG System ready for questions!\")\n",
    "    system_ready = True\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up system: {e}\")\n",
    "    print(\"Please check your file path and API key\")\n",
    "    system_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Question-Answer Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_simple_question(question, use_hyde=True):\n",
    "    \"\"\"Simple function to ask a question and print results\"\"\"\n",
    "    if not system_ready:\n",
    "        print(\"❌ System not ready. Please check setup above.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🤔 Question: {question}\")\n",
    "    print(f\"🔬 Using HyDE: {'Yes' if use_hyde else 'No'}\")\n",
    "    print(\"⏳ Processing...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Get detailed results\n",
    "        result = rag_system.ask_detailed(question, use_hyde=use_hyde)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📝 ANSWER:\")\n",
    "        print(\"=\"*80)\n",
    "        print(result[\"answer\"])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 DETAILS:\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"⏱️  Processing time: {processing_time:.2f} seconds\")\n",
    "        print(f\"📄 Documents retrieved: {len(result['retrieved_documents'])}\")\n",
    "        print(f\"🎯 Documents after reranking: {len(result['reranked_documents'])}\")\n",
    "        \n",
    "        if result['hyde_document'] and use_hyde:\n",
    "            print(f\"\\n🔮 HyDE Generated Document (first 200 chars):\")\n",
    "            print(f\"{result['hyde_document']}...\")\n",
    "        \n",
    "        print(f\"\\n📋 Top Retrieved Document Snippets:\")\n",
    "        for i, doc in enumerate(result['reranked_documents'][:3], 1):\n",
    "            print(f\"\\n[{i}] Score: {doc.score:.4f}\")\n",
    "            print(f\"Content: {doc.content}...\")\n",
    "\n",
    "        return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing question: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Example usage:\n",
    "print(\"✅ Simple Q&A function ready!\")\n",
    "print(\"📝 Use: ask_simple_question('Your question here')\")\n",
    "print(\"🔬 With HyDE: ask_simple_question('Your question', use_hyde=True)\")\n",
    "print(\"🚀 Without HyDE: ask_simple_question('Your question', use_hyde=False)\")\n",
    "\n",
    "# %%\n",
    "# Try asking a question:\n",
    "result = ask_simple_question(\"Jaka liczba pracowników jest zatrudniona przez firmę?\")\n",
    "\n",
    "# %%\n",
    "# Ask another question without HyDE:\n",
    "result_no_hyde = ask_simple_question(\"Jaka liczba pracowników jest zatrudniona przez firmę?\", use_hyde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['reranked_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_no_hyde['reranked_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
